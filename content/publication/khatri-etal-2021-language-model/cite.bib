@inproceedings{khatri-etal-2021-language-model,
 abstract = {This paper describes our submission for the shared task on Unsupervised MT and Very Low Resource Supervised MT at WMT 2021. We submitted systems for two language pairs: German ↔ Upper Sorbian (de ↔ hsb) and German-Lower Sorbian (de ↔ dsb). For de ↔ hsb, we pretrain our system using MASS (Masked Sequence to Sequence) objective and then finetune using iterative back-translation. Final finetunng is performed using the parallel data provided for translation objective. For de ↔ dsb, no parallel data is provided in the task, we use final de ↔ hsb model as initialization of the de ↔ dsb model and train it further using iterative back-translation, using the same vocabulary as used in the de ↔ hsb model.},
 address = {Online},
 author = {Khatri, Jyotsana  and
Murthy, Rudra  and
Bhattacharyya, Pushpak},
 booktitle = {Proceedings of the Sixth Conference on Machine Translation},
 month = {November},
 pages = {995--998},
 publisher = {Association for Computational Linguistics},
 title = {Language Model Pretraining and Transfer Learning for Very Low Resource Languages},
 url = {https://aclanthology.org/2021.wmt-1.106},
 year = {2021}
}

